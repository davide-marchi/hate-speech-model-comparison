\documentclass[conference]{IEEEtran}

% --- Packages ---
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{cite}
\usepackage{booktabs}
\usepackage{csvsimple}
\usepackage{etoolbox}
\usepackage{siunitx}
\usepackage[hidelinks]{hyperref}
\usepackage{url}

% For compact URLs in footnotes
\def\UrlBreaks{\do\/\do-}

% siunitx rounding setup for tables
\sisetup{round-mode=places,round-precision=3,detect-all=true,output-exponent-marker=\mathrm{e}}

% Map CSV model identifiers to printable names (avoid underscores)
\newcommand{\modelname}[1]{%
  \ifstrequal{#1}{svm_tfidf}{SVM+TF--IDF}{%
  \ifstrequal{#1}{minilm_logreg}{MiniLM+LR}{%
  \ifstrequal{#1}{lstm_skorch}{LSTM}{%
  \ifstrequal{#1}{regex_keyword}{Regex/Keyword}{%
  \ifstrequal{#1}{zero_shot}{Zero-shot}{\texttt{\detokenize{#1}}}%
  }}}}}

% --- Title/Author ---
\begin{document}
\title{Hate Speech Classification: Effectiveness--Efficiency Trade-offs on Twitter}
\author{\IEEEauthorblockN{Davide Marchi}\\\IEEEauthorblockA{Master's Degree Student, University of Pisa}\\\IEEEauthorblockA{Email: d.marchi5@studenti.unipi.it}}
\maketitle

% --- Abstract ---
\begin{abstract}
We compare five text classifiers for hate speech detection on Twitter: a regex/keyword baseline, linear SVM with TF--IDF features, MiniLM sentence embeddings with logistic regression, an LSTM sequence model, and a zero-shot classifier based on DistilBERT fine-tuned on MNLI. Effectiveness is measured by macro-F1 and accuracy; efficiency is measured by estimated carbon emissions (kg~CO$_2$e) for training and testing, alongside time. Pairwise McNemar tests assess statistical significance. A Pareto analysis of macro-F1 vs test emissions highlights practical trade-offs between effectiveness and environmental cost.
\end{abstract}

\section{Introduction}
We study the effectiveness--efficiency trade-off in hate speech classification with five representative models, from keyword rules to pretrained transformers. We report macro-F1 and accuracy alongside estimated emissions, and test for significance with McNemar's test. Code and artifacts: \href{https://github.com/davide-marchi/hate-speech-model-comparison}{github.com/davide-marchi/hate-speech-model-comparison}.

\section{Data}
We use the Twitter hate speech dataset (training split) as provided on Kaggle \cite{kaggle_twitter_hate_speech}. Table~\ref{tab:dist} shows class distribution with token length statistics. We apply standard text cleaning (URLs, mentions, lowercasing).

% (Dataset overview table removed as requested.)

\begin{table}[!t]
\caption{Label distribution and token length in the training data.}
\label{tab:dist}
\centering
\small
\csvreader[
    tabular=rrrrr,
    table head=\toprule label & count & percent & tokens mean & tokens std \\ \midrule,
    late after line=\\,
    table foot=\bottomrule
]{../results/analysis/per_class_summary.csv}{}{
    \csvcoli & \csvcolii & \num{\csvcoliii} & \num{\csvcolxvii} & \num{\csvcolxviii}
}
\end{table}

\section{Methods}
We compare the following models:
\begin{itemize}
    \item \textbf{Regex/keyword}: a lexical baseline built from indicative keywords and patterns; see early rule-based toxic language detection \cite{spertus1997smokey}.
    \item \textbf{SVM + TF--IDF}: a linear SVM on word/character TF--IDF features, a strong classical baseline for text classification \cite{joachims1998text}.
    \item \textbf{MiniLM + Logistic Regression}: sentence embeddings from \texttt{all-MiniLM-L6-v2} \cite{wang2020minilm} followed by logistic regression.
    \item \textbf{LSTM}: a recurrent model over token sequences \cite{hochreiter1997long}.
    \item \textbf{Zero-shot (DistilBERT-MNLI)}: using the pretrained model \texttt{distilbert-base-uncased-mnli} \cite{sanh2019distilbert} for zero-shot classification via NLI.
\end{itemize}
We perform grid search over model-specific hyperparameters; performance is reported on the test set (see Table~\ref{tab:summary}).

\section{Experimental Setup}
All runs are performed on CPU only for consistency across models. We track both \emph{time} and \emph{carbon emissions (kg~CO$_2$e)} during training and testing. Emissions are estimated via a tracker implemented with the CodeCarbon library \cite{codecarbon}, which accounts for runtime and power draw; therefore, we emphasize emissions as an efficiency metric rather than wall-clock time alone \cite{lacoste2019quantifying}.

\section{Results}
\subsection{Effectiveness and Emissions}
Table~\ref{tab:summary} reports test macro-F1 and accuracy alongside estimated test and train emissions. The SVM+TF--IDF model achieves the highest macro-F1 while keeping emissions low relative to neural alternatives. The keyword baseline is extremely efficient but lags in macro-F1. The zero-shot DistilBERT provides modest macro-F1 and comparatively higher emissions.

\begin{table*}[!t]
\caption{Effectiveness and emissions summary.}
\label{tab:summary}
\centering
\small
\csvreader[
    tabular=lcccccc,
    table head=\toprule model & macro-F1 & accuracy & test~time~(s) & train~time~(s) & test~emissions~(kg) & train~emissions~(kg) \\\midrule,
    late after line=\\,
    table foot=\bottomrule
]{../results/analysis/summary.csv}{}{
    \expandafter\modelname\expandafter{\csvcoli} & \num{\csvcolx} & \num{\csvcolvii} & \num{\csvcolv} & \num{\csvcoliii} & \num[round-precision=6]{\csvcolvi} & \num[round-precision=6]{\csvcoliv}
}
\end{table*}

Figure~\ref{fig:pareto} shows the Pareto frontier of macro-F1 vs test emissions. We prefer emissions-based comparisons over time-based ones, given emissions encode both runtime and CPU utilization under our CPU-only protocol.

\begin{figure}[!t]
    \centering
    \includegraphics[width=\linewidth]{../results/analysis/pareto_test_emissions.png}
    \caption{Macro-F1 vs test emissions with a Pareto frontier.}
    \label{fig:pareto}
\end{figure}

\subsection{Statistical Significance}
We apply McNemar's test to paired predictions. Table~\ref{tab:mcnemar} lists pairwise results; SVM+TF--IDF significantly outperforms the neural baselines at $\alpha=0.05$ on this test set.

\begin{table}[!t]
\caption{McNemar pairwise tests (p-value and significance).}
\label{tab:mcnemar}
\centering
\small
\csvreader[
    tabular=llcc,
    table head=\toprule model\_a & model\_b & p\_value & significant \\\midrule,
    late after line=\\,
    table foot=\bottomrule
]{../results/analysis/significance_mcnemar.csv}{}{
    \expandafter\modelname\expandafter{\csvcoli} & \expandafter\modelname\expandafter{\csvcolii} & \num{\csvcolviii} & \csvcolix
}
\end{table}

\subsection{Error Analysis}
We inspect confusion matrices and misclassified examples. Due to space, we include the SVM+TF--IDF confusion matrix in Figure~\ref{fig:cm-svm}; other model-specific reports are similar and are available in the artifacts.

\begin{figure}[!t]
    \centering
    \includegraphics[width=.5\linewidth]{../results/svm_tfidf/analysis/confusion_matrix.png}
    \caption{Confusion matrix for SVM+TF--IDF on the test set.}
    \label{fig:cm-svm}
\end{figure}

\section{Discussion}
Our results suggest that strong linear baselines remain competitive on short social media text: SVM+TF--IDF provides the best macro-F1 at a favorable emissions footprint. Pretrained embeddings (MiniLM) and LSTM narrow the effectiveness gap but incur higher emissions per test example. Zero-shot classification offers convenience and label flexibility without task-specific training, but its emissions are comparatively higher under CPU-only inference. The keyword baseline is the most efficient but sacrifices recall.

Generalization to other hate speech datasets is plausible for relative rankings between simple linear baselines and larger neural models, though dataset shift (lexical, topical, and label schema) can alter absolute metrics. The emissions trends should persist under CPU-only execution; on GPUs, relative emissions may change depending on batch size and utilization.

\section{Limitations}
Class imbalance and domain-specific language may bias metrics. Emissions are estimates (e.g., based on hardware utilization and regional energy intensity) and should be interpreted as relative comparisons rather than exact measurements. We restrict to small models and CPU to keep runs comparable; larger models or GPUs may change the Pareto frontier.

\section{Conclusion}
On this dataset, SVM+TF--IDF is a strong effectiveness--efficiency choice, dominating neural alternatives under CPU-only constraints. Emissions-aware evaluation provides a more informative perspective than time alone because it incorporates utilization. Code and full artifacts are available at \href{https://github.com/davide-marchi/hate-speech-model-comparison}{github.com/davide-marchi/hate-speech-model-comparison}.

\begin{thebibliography}{00}
\bibitem{spertus1997smokey}
E. Spertus, ``Smokey: Automatic Recognition of Hostile Messages,'' in IAAI, 1997.

\bibitem{joachims1998text}
T. Joachims, ``Text Categorization with Support Vector Machines: Learning with Many Relevant Features,'' in ECML, 1998.

\bibitem{wang2020minilm}
W. Wang, F. Wei, L. Dong, H. Bao, N. Yang, and M. Zhou, ``MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers,'' arXiv:2002.10957, 2020.

\bibitem{hochreiter1997long}
S. Hochreiter and J. Schmidhuber, ``Long short-term memory,'' Neural Computation, 9(8):1735--1780, 1997.

\bibitem{sanh2019distilbert}
V. Sanh, L. Debut, J. Chaumond, and T. Wolf, ``DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter,'' arXiv:1910.01108, 2019.

\bibitem{lacoste2019quantifying}
A. Lacoste, A. Sasha Luccioni, V. Schmidt, and T. Dandres, ``Quantifying the Carbon Emissions of Machine Learning,'' arXiv:1910.09700, 2019.

\bibitem{codecarbon}
CodeCarbon, ``CodeCarbon: Track Carbon Emissions from Machine Learning,'' available at: \url{https://github.com/mlco2/codecarbon} (accessed Nov. 2025).

\bibitem{kaggle_twitter_hate_speech}
``Twitter Hate Speech,'' Kaggle, available at: \url{https://www.kaggle.com/vkrahul/twitter-hate-speech} (accessed Nov. 2025).
\end{thebibliography}

\end{document}
