\documentclass[conference]{IEEEtran}

% --- Packages ---
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{cite}
\usepackage{booktabs}
\usepackage{csvsimple}
\usepackage{etoolbox}
\usepackage{siunitx}
\usepackage[hidelinks]{hyperref}
\usepackage{url}

% For compact URLs in footnotes
\def\UrlBreaks{\do\/\do-}

% siunitx rounding setup for tables
\sisetup{round-mode=places,round-precision=3,detect-all=true,output-exponent-marker=\mathrm{e}}

% Map CSV model identifiers to printable names (avoid underscores)
\newcommand{\modelname}[1]{%
  \ifstrequal{#1}{svm_tfidf}{SVM+TF-IDF}{%
  \ifstrequal{#1}{minilm_logreg}{MiniLM+LR}{%
  \ifstrequal{#1}{lstm_skorch}{LSTM}{%
  \ifstrequal{#1}{regex_keyword}{Regex/Keyword}{%
  \ifstrequal{#1}{zero_shot}{Zero-shot}{\texttt{\detokenize{#1}}}%
  }}}}}

% Map numeric labels to readable names for tables (robust to stray spaces)
\newcommand{\labname}[1]{%
  \begingroup
    \edef\labtmp{#1}% ensure full expansion
    \ifnum\numexpr\labtmp\relax=0 Not hateful% 0 -> not hateful
    \else\ifnum\numexpr\labtmp\relax=1 Hateful% 1 -> hateful
    \else #1% fallback: print as-is for unknown labels
    \fi\fi
  \endgroup
}

% --- Title/Author ---
\begin{document}
\title{Hate Speech Classification: Effectiveness-Efficiency Trade-offs on Twitter}
\author{\IEEEauthorblockN{Davide Marchi}\\\IEEEauthorblockA{Master's Degree Student, University of Pisa}\\\IEEEauthorblockA{Email: d.marchi5@studenti.unipi.it}}
\maketitle

% --- Abstract ---
\begin{abstract}
We compare five text classifiers for hate speech detection on Twitter: a regex/keyword baseline, linear SVM with TF-IDF features, MiniLM sentence embeddings with logistic regression, an LSTM sequence model, and a zero-shot classifier based on DistilBERT fine-tuned on MNLI. Effectiveness is measured by macro-F1 and accuracy; efficiency is measured by estimated carbon emissions (kg~CO$_2$e) for training and testing, alongside time. Pairwise McNemar tests assess statistical significance. A Pareto analysis of macro-F1 vs test emissions highlights practical trade-offs between effectiveness and environmental cost.
\end{abstract}

\section{Introduction}
We study the effectiveness-efficiency trade-off in hate speech classification with five representative models, from keyword rules to pretrained transformers. We report macro-F1 and accuracy alongside estimated emissions, and test for significance with McNemar's test. Code and artifacts: \href{https://github.com/davide-marchi/hate-speech-model-comparison}{github.com/davide-marchi/hate-speech-model-comparison}.

\section{Data}
We use the Twitter hate speech dataset (training split) as provided on Kaggle \cite{kaggle_twitter_hate_speech}. Table~\ref{tab:dist} shows class distribution with token length statistics. We apply standard text cleaning (URLs, mentions, lowercasing).

% (Dataset overview table removed as requested.)

\begin{table}[!t]
\caption{Label distribution and token length in the training data.}
\label{tab:dist}
\centering
\footnotesize
\csvreader[
    tabular=rrrrr,
    table head=\toprule label & count & percent & tokens mean & tokens std \\ \midrule,
    late after line=\\,
    table foot=\bottomrule
]{../results/analysis/per_class_summary.csv}{}{
    \labname{\csvcoli} & \csvcolii & \num{\csvcoliii} & \num{\csvcolxvii} & \num{\csvcolxviii}
}
\end{table}

\section{Methods}
We compare the following models:
\begin{itemize}
    \item \textbf{Regex/keyword}: a lexical baseline built from indicative keywords and patterns; see early rule-based toxic language detection \cite{spertus1997smokey}.
    \item \textbf{SVM + TF-IDF}: a linear SVM on word/character TF-IDF features, a strong classical baseline for text classification \cite{joachims1998text}.
    \item \textbf{MiniLM + Logistic Regression}: sentence embeddings from \texttt{all-MiniLM-L6-v2} \cite{wang2020minilm} followed by logistic regression.
    \item \textbf{LSTM}: a recurrent model over token sequences \cite{hochreiter1997long}.
    \item \textbf{Zero-shot (DistilBERT-MNLI)}: using the pretrained model \texttt{distilbert-base-uncased-mnli} \cite{sanh2019distilbert} for zero-shot classification via NLI.
\end{itemize}
We perform grid search over model-specific hyperparameters; performance is reported on the test set (see Table~\ref{tab:summary}).

\section{Experimental Setup}
All runs are performed on CPU only for consistency across models. We track both \emph{time} and \emph{carbon emissions (kg~CO$_2$e)} during training and testing. Emissions are estimated via a tracker implemented with the CodeCarbon library \cite{codecarbon}, which accounts for runtime and power draw; therefore, we emphasize emissions as an efficiency metric rather than wall-clock time alone \cite{lacoste2019quantifying}.

\section{Results}
\subsection{Effectiveness and Emissions}
Table~\ref{tab:summary} reports test macro-F1, accuracy, test/train time and emissions, plus an efficiency metric (F1 per kg CO$_2$e) for testing. The SVM+TF-IDF model achieves the highest macro-F1 while keeping emissions low relative to neural alternatives. The keyword baseline is the least energy consuming but lags in macro-F1. The zero-shot DistilBERT provides modest macro-F1 and comparatively higher emissions.

\begin{table*}[!t]
\caption{Effectiveness and emissions summary.}
\label{tab:summary}
\centering
\footnotesize
\csvreader[
    tabular=lccccccc,
    table head=\toprule model & macro-F1 & accuracy & test~time~(s) & train~time~(s) & test~emissions~(kg) & train~emissions~(kg) & efficiency~(F1/kg) \\\midrule,
    late after line=\\,
    table foot=\bottomrule
]{../results/analysis/summary.csv}{}{
    \expandafter\modelname\expandafter{\csvcoli} & \num{\csvcolx} & \num{\csvcolvii} & \num{\csvcolv} & \num{\csvcoliii} & \num[round-precision=6]{\csvcolvi} & \num[round-precision=6]{\csvcoliv} & \num{\csvcolxiv}
}
\end{table*}

Figure~\ref{fig:pareto} shows the Pareto frontier of macro-F1 vs test emissions.

\begin{figure}[!b]
    \centering
    \includegraphics[width=\linewidth]{../results/analysis/pareto_test_emissions.png}
    \caption{Macro-F1 vs test emissions with a Pareto frontier.}
    \label{fig:pareto}
\end{figure}

\subsection{Statistical Significance}
We apply McNemar's test to paired predictions. Table~\ref{tab:mcnemar} lists pairwise results. The only comparison that is not significant at $\alpha=0.05$ is between LSTM and MiniLM+LR ($p\approx0.627$), meaning we cannot conclude a true difference in performance despite the small macro-F1 gap. All other reported pairwise differences are significant; in particular, SVM+TF-IDF significantly outperforms all the alternatives.

\begin{table}[!t]
\caption{McNemar pairwise tests (p-value and significance).}
\label{tab:mcnemar}
\centering
\footnotesize
\csvreader[
    tabular=llcc,
    table head=\toprule model\_a & model\_b & p\_value & significant \\\midrule,
    late after line=\\,
    table foot=\bottomrule
]{../results/analysis/significance_mcnemar.csv}{}{
    \expandafter\modelname\expandafter{\csvcoli} & \expandafter\modelname\expandafter{\csvcolii} & \num{\csvcolviii} & \csvcolix
}
\end{table}

\subsection{Error Analysis}
Beyond the metrics reported here, each classifier's confusion matrix and the list of misclassified examples are available in the repository. For illustration, we show the confusion matrix of the best-performing classifier (SVM+TF-IDF) in Figure~\ref{fig:cm-svm}.

\begin{figure}[!t]
    \centering
    \includegraphics[width=.5\linewidth]{../results/svm_tfidf/analysis/confusion_matrix.png}
    \caption{Confusion matrix for SVM+TF-IDF on the test set.}
    \label{fig:cm-svm}
\end{figure}

\section{Discussion}
Our results suggest that strong linear baselines remain competitive on short social media text: SVM+TF-IDF provides the best macro-F1 at a favorable emissions footprint. Pretrained embeddings (MiniLM) and LSTM narrow the effectiveness gap but incur higher emissions per test example. Zero-shot classification offers convenience and label flexibility without task-specific training, but its emissions are comparatively higher. The keyword baseline is the least energy consuming.

Generalization to other hate speech datasets without retraining is plausible, but sensitivity to context matters. When syntax and lexicon shift substantially, syntax-based approaches such as regex and SVM+TF-IDF are likely to degrade most, while semantic approaches like sentence embeddings and LSTM (which leverages pretrained word embeddings) should be more robust.

\section{Limitations}
Neural models such as LSTM, zero-shot transformers, and sentence-embedding pipelines can scale far beyond what we ran here: many hyperparameters, larger architectures, and heavier training that often improve accuracy but raise emissions. Due to hardware limits, we explored small configurations, whereas the SVM+TF-IDF baseline is near its optimal regime on this dataset. With more compute and a larger corpus, SVM+TF-IDF might improve slightly, but the other approaches could surpass it, at a higher carbon footprint.

\section{Conclusion}
On this dataset, SVM+TF-IDF is a strong effectiveness-efficiency choice, while with sufficient time and compute fully fine-tuned semantic models (e.g., BERT) would likely outperform it and generalize better across datasets.

\begin{thebibliography}{00}
\bibitem{kaggle_twitter_hate_speech}
``Twitter Hate Speech,'' Kaggle, available at: \url{https://www.kaggle.com/vkrahul/twitter-hate-speech} (accessed Nov. 2025).

\bibitem{spertus1997smokey}
E. Spertus, ``Smokey: Automatic Recognition of Hostile Messages,'' in IAAI, 1997.

\bibitem{joachims1998text}
T. Joachims, ``Text Categorization with Support Vector Machines: Learning with Many Relevant Features,'' in ECML, 1998.

\bibitem{wang2020minilm}
W. Wang, F. Wei, L. Dong, H. Bao, N. Yang, and M. Zhou, ``MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers,'' arXiv:2002.10957, 2020.

\bibitem{hochreiter1997long}
S. Hochreiter and J. Schmidhuber, ``Long short-term memory,'' Neural Computation, 9(8):1735-1780, 1997.

\bibitem{sanh2019distilbert}
V. Sanh, L. Debut, J. Chaumond, and T. Wolf, ``DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter,'' arXiv:1910.01108, 2019.

\bibitem{codecarbon}
CodeCarbon, ``CodeCarbon: Track Carbon Emissions from Machine Learning,'' available at: \url{https://github.com/mlco2/codecarbon} (accessed Nov. 2025).

\bibitem{lacoste2019quantifying}
A. Lacoste, A. Sasha Luccioni, V. Schmidt, and T. Dandres, ``Quantifying the Carbon Emissions of Machine Learning,'' arXiv:1910.09700, 2019.
\end{thebibliography}

\end{document}
